#-----------------------------------------------------------------------------------------------------#
# Root Mean Squared Error
RMSE = function(yact, ypred){
sqrt(mean((yact - ypred)^2))
}
# Mean Absolute Error
MAE = function(yact, ypred){
mean(abs(yact - ypred))
}
#-----------------------------------------------------------------------------------------------------#
#		Block 03		Get query results
#-----------------------------------------------------------------------------------------------------#
# Set endpoint for wikidata
endpoint = "https://query.wikidata.org/bigdata/namespace/wdq/sparql"
# Get the results
queryResults = query_wikidata(query)
#-----------------------------------------------------------------------------------------------------#
#		Block 03		bp check ; to kelvin
#-----------------------------------------------------------------------------------------------------#
#C to Kelvin:
# 0°C + 273.15 = 273,15K
indexCelciusResults = which(queryResults$bpUnitLabel=="degree Celsius")
for( i in 1:length(indexCelciusResults)){
queryResults[indexCelciusResults[i],]$bp = (queryResults[indexCelciusResults[i],]$bp + 273.15)
queryResults[indexCelciusResults[i],]$bpUnitLabel = "kelvin"
}
#F to Kelvin:
# (0°F − 32) × 5/9 + 273.15 = 255,372K
indexFahrenheitResults = which(queryResults$bpUnitLabel=="degree Fahrenheit")
for( i in 1:length(indexFahrenheitResults)){
queryResults[indexFahrenheitResults[i],]$bp= (queryResults[indexFahrenheitResults[i],]$bp - 32) * (5/9) + 273.15
queryResults[indexFahrenheitResults[i],]$bpUnitLabel = "kelvin"
}
#additional failsafe if other metrics than Celsius and Fahrenheit are added (other metrics are removed)
queryResults = queryResults[which(queryResults$bpUnitLabel=="kelvin"),]
# Make backup
queryResultsBACKUP = queryResults
# Finalize data structure
queryResults = data.frame(queryResultsBACKUP$compLabel, queryResultsBACKUP$bp, queryResultsBACKUP$CC)
names(queryResults) = c("Comp","bp","CC")
#-----------------------------------------------------------------------------------------------------#
#		Block 03		FILTERS AND OUTLIER HANDLING
#-----------------------------------------------------------------------------------------------------#
if(linearAlkanesOnly == 1){
queryResults = queryResults[-grep(pattern = "\\(",x = queryResults$CC),]
}
# hexatriacontane 770.15 K (497 C); in wikidata under pressurised condition!
if(!length(queryResults$Comp=="hexatriacontane")==0){
queryResults$bp[queryResults$Comp=="hexatriacontane"] = 770.15
}
# Dooctacontane 958.05 K (684.9 C); IN WIKIDATA AS 881.85 K
if(!length(queryResults$Comp=="Dooctacontane")==0){
queryResults$bp[queryResults$Comp=="Dooctacontane"] = 958.05
}
# phytane 612.32 K (322.4 C); IN WIKIDATA AS 442.65 K at 760 torr which should be (322.4 C) 612.32 K.
if(!length(queryResults$Comp=="phytane")==0){
queryResults$bp[queryResults$Comp=="phytane"] = 612.32
}
#-----------------------------------------------------------------------------------------------------#
#		Block 03		DATA VIZ
#-----------------------------------------------------------------------------------------------------#
# Get a general idea of how the data looks; disregarding branch effects; amount of C in compound linked to BP
CClength_crude = nchar(gsub(pattern = "\\)",replacement = "",x = gsub(pattern = "\\(",replacement = "",x = queryResults$CC)))
plotCClength = CClength_crude[order(CClength_crude)]
plotBP = queryResults$bp[order(CClength_crude)]
# Should result in a exponential function-like graph
plot(plotCClength,plotBP,main = "Carbon - boilingpoint relation",xlab = "Amount of carbon atoms in alkene",ylab = "Boiling point (Kelvin)")
# Show how the data is distributed (focussing on bp)
hist(queryResults$bp,breaks=20,main = "Boiling point frequency distribution",xlab = "Boiling point (Kelvin)",ylab = "Frequency")
#-----------------------------------------------------------------------------------------------------#
# 		Block 03		rcdk data extraction see:https://cran.r-project.org/web/packages/rcdk/vignettes/molform.html
#-----------------------------------------------------------------------------------------------------#
smilesParser <- get.smiles.parser()
descCategories <- get.desc.categories()
for( i in 1:length(queryResults$CC)){
# Get smiles from result query and add information
selectedSmilesData <- parse.smiles(queryResults$CC[i])[[1]]
convert.implicit.to.explicit(selectedSmilesData)
#formula <- get.mol2formula(selectedSmilesData,charge=0)
# Store the found information
#queryResults$formula[i] = {formula} # S4 object cannot be transferred nicely
#queryResults$mass[i] = formula@mass
#queryResults$string[i] = formula@string
#queryResults$charge[i] = formula@charge
# M/z values
#queryResults$isotopes[i] = {get.isotopes.pattern(formula,minAbund=0.1)}
# Fingerprint? values
#queryResults$fingerprint[i] = {get.fingerprint(selectedSmilesData = selectedSmilesData)}
# Create a dataframe which contains all info possible to extract using the descriptors
rowSmileDescData = queryResults$Comp[i]
for(o in 1:5){
dn <- get.desc.names(descCategories[o])
rowSmileDescData = cbind(rowSmileDescData, eval.desc(selectedSmilesData, dn))
}
# This is done now as it will break if descriptors change (amount)
if(exists("dfSmilesDescData")){
dfSmilesDescData[i,] = rowSmileDescData
}else{
dfSmilesDescData = rowSmileDescData
}
}
# Make backup from data, easy for testing (resetting)
mydataBACKUP = dfSmilesDescData
dfSmilesDescData = mydataBACKUP
# Remove component names (as it will mess up futher processing)
dfSmilesDescData = dfSmilesDescData[,-1]
#-----------------------------------------------------------------------------------------------------#
#							Latent variable selection (correlation)
#-----------------------------------------------------------------------------------------------------#
# Remove NAs n stuff
dfSmilesDescData <- dfSmilesDescData[, !apply(dfSmilesDescData, 2, function(x) any(is.na(x)) )]
dfSmilesDescData <- dfSmilesDescData[, !apply( dfSmilesDescData, 2, function(x) length(unique(x)) == 1 )]
if(T){
# Correlate the descriptors with the boiling point; if these are linked, they should add some info
corMatrix = cor(dfSmilesDescData , queryResults$bp)
corMatrix = as.data.frame(corMatrix[order(abs(corMatrix),decreasing = T),])
# select first 15 components:
componentNames = rownames(corMatrix)[1:15]
dfSmilesDescData = dfSmilesDescData[,match(colnames(dfSmilesDescData), x = componentNames)]
}
# - BLOCKED - Old method, keepin this inside for further reference
if(F){
dfSmilesDescData = mydataBACKUP
dfSmilesDescData = dfSmilesDescData[,-1]
# crude way to extract 'important' features based on correlation
dfSmilesDescData <- dfSmilesDescData[, !apply(dfSmilesDescData, 2, function(x) any(is.na(x)) )]
dfSmilesDescData <- dfSmilesDescData[, !apply( dfSmilesDescData, 2, function(x) length(unique(x)) == 1 )]
r2 <- which(cor(dfSmilesDescData)^2 > .9, arr.ind=TRUE) # when keeping this high, the prediction improves
r2 <- r2[ r2[,1] > r2[,2] , ]
dfSmilesDescData <- dfSmilesDescData[, -unique(r2[,2])]
}
# - BLOCKED -
# should contain nAtomLAC; the amount of c atoms
if(!exists("dfSmilesDescData$nAtomLAC")){
dfSmilesDescData = cbind(dfSmilesDescData, dfSmilesDescData$nAtomLAC)
names(dfSmilesDescData)[dim(dfSmilesDescData)[2]] = "n"
}else{
names(dfSmilesDescData)[names(dfSmilesDescData)=="dfSmilesDescData$nAtomLAC"] = "n"
}
# Store resulting input file in dfInputML; as it is the input for ML
dfInputML = dfSmilesDescData
# Finish prepare data
# Add and define the 'to be predicted' column
indexBoilPoint = ncol(dfInputML)+1
dfInputML[,indexBoilPoint] = queryResults$bp
names(dfInputML)[indexBoilPoint] = 'BoilPoint'
# Ordered data is required later
dfInputML = dfInputML[order(dfInputML$BoilPoint),]
#-----------------------------------------------------------------------------------------------------#
#							MACHINE LEARNING
#-----------------------------------------------------------------------------------------------------#
#-----------------------------------------------------------------------------------------------------#
# 				Data subsetting; Train; Test
#-----------------------------------------------------------------------------------------------------#
# Define variables
sample.length = length(dfInputML[,1])
# Higher probability to select lower (underrepresented) samples;
# improves range of model, increases fit and prediction power.
# Ranges from 1 (select) to 0.5 (chance)
sample.biasprob = 1 - 1:sample.length /max(sample.length )/2
# Make data objects
samples.upper = sample(sample.length , floor(length(dfInputML[,1])*trainingTestRatio),prob = sample.biasprob ) #get all unique samples (not frequecies) and sample 80%
#plot(samples.upper[order(samples.upper)])
samples.total = (1:length(dfInputML[,1])) # all unique samples (not frequecies)
samples.lowerl = samples.total[!samples.total %in% samples.upper]  #which samples are sampled
# Subset data
data.train = dfInputML[samples.upper,]#contains all upper
data.train = as.data.frame(data.train)
data.test = dfInputML[samples.lowerl,]#contains all lower
data.test = as.data.frame(data.test)
# Remove nas
data.train=data.train[!is.na(data.train[,1]),]
data.test=data.test[!is.na(data.test[,1]),]
# Create standardized object
xNN = data.train[,-(indexBoilPoint)]
yNN = data.train[,indexBoilPoint]
dat = data.frame(xNN, y = yNN)
# Remove samples with in datasets n < 1
data.train = dat[dat$n>0,]
yactual.train = data.train$y
data.test = data.test[data.test$n>0,]
yactual.test = data.test$BoilPoint
#-----------------------------------------------------------------------------------------------------#
#							CARET : pls
#-----------------------------------------------------------------------------------------------------#
# Define training control method; 10 - k - cross validation
train_control <- trainControl(method="cv", number=10)
# Train the model
model <- train(y~., data=data.train, trControl=train_control, method="pls")
# Find out what model is best
print(model)
# Find out most important variables
Varimportance = varImp(model)
cat(paste("Best model fit with", model$bestTune, "latent components \n"))
cat(paste("Latent components:",paste(rownames(Varimportance$importance)[order(decreasing = T,Varimportance$importance$Overall)][1:model$bestTune[1,]],collapse = ", "),"\n"))
plot(Varimportance, main="Varible importance in PLS model \n")
# Predict test set
ypredCARET.pls.test <- model %>% predict(data.test)
# Root mean squared error
RMSE.pls.test = RMSE(yactual.test, ypredCARET.pls.test)
RMSE.pls.test
# Results in: 16.09
# Mean absolute error
MAE.pls.test = MAE(yactual.test, ypredCARET.pls.test)
MAE.pls.test
# Results in: 14.97
# Plot ypred vs yactual of test data
plot(yactual.test, ypredCARET.pls.test,
xlab="Observed BP test set", ylab="Predicted BP test set",
pch=19, xlim=c(0, ceiling(max(yNN)*1.1)), ylim=c(0, ceiling(max(yNN)*1.1)),main="Prediction error test set (PLS model)")
abline(0,1, col='red')
text(200,ceiling(max(yNN)*1.1),paste("RMSE: ",RMSE.pls.test))
text(200,ceiling(max(yNN)*1.1)-50,paste("MAE: ",MAE.pls.test))
# Predict training data; check overfitting
ypredCARET.pls.train <- model %>% predict(data.train)
# Root mean squared error
RMSE.pls.train = RMSE(yactual.train, ypredCARET.pls.train)
RMSE.pls.train
# Results in: 19.94
# Mean absolute error
MAE.pls.train = MAE(yactual.train, ypredCARET.pls.train)
MAE.pls.train
# Results in: 14.84
# Plot ypred vs yactual of training data
plot(yactual.train, ypredCARET.pls.train,
xlab="Observed BP train set", ylab="Predicted BP train set",
pch=19, xlim=c(0, ceiling(max(yNN)*1.1)), ylim=c(0, ceiling(max(yNN)*1.1)),main="Prediction error training set (PLS model)")
abline(0,1, col='red')
text(200,ceiling(max(yNN)*1.1),paste("RMSE: ",RMSE.pls.train))
text(200,ceiling(max(yNN)*1.1)-50,paste("MAE: ",MAE.pls.train))
#-----------------------------------------------------------------------------------------------------#
#							CARET : RF
#-----------------------------------------------------------------------------------------------------#
# Define training control method; 10 - k - cross validation
train_control <- trainControl(method="cv", number=10)
# Train the model
model <- train(y~., data=data.train, trControl=train_control, method="rf")
# Find out what model is best
print(model)
# Find out most important variables - BLOCKED - this doesnt work for rf
if(F){
Varimportance = varImp(model)
cat(paste("Best model fit with", model$bestTune, "latent components \n"))
cat(paste("Latent components:",paste(rownames(Varimportance$importance)[order(decreasing = T,Varimportance$importance$Overall)][1:model$bestTune[1,]],collapse = ", "),"\n"))
plot(Varimportance, main="Varible importance in rf model \n")
}
# Predict test set
ypredCARET.rf.test <- model %>% predict(data.test)
# Root mean squared error
RMSE.rf.test = RMSE(yactual.test, ypredCARET.rf.test)
RMSE.rf.test
# Results in: 8.19
# Mean absolute error
MAE.rf.test = MAE(yactual.test, ypredCARET.rf.test)
MAE.rf.test
# Results in: 5.35
# Plot ypred vs yactual of test data
plot(yactual.test, ypredCARET.rf.test,
xlab="Observed BP test set", ylab="Predicted BP test set",
pch=19, xlim=c(0, ceiling(max(yNN)*1.1)), ylim=c(0, ceiling(max(yNN)*1.1)),main="Prediction error test set (RandomForest model)")
abline(0,1, col='red')
text(200,ceiling(max(yNN)*1.1),paste("RMSE: ",RMSE.rf.test))
text(200,ceiling(max(yNN)*1.1)-50,paste("MAE: ",MAE.rf.test))
# Predict training data; check overfitting
ypredCARET.rf.train <- model %>% predict(data.train)
# Root mean squared error
RMSE.rf.train = RMSE(yactual.train, ypredCARET.rf.train)
RMSE.rf.train
# Results in: 7.65
# Mean absolute error
MAE.rf.train = MAE(yactual.train, ypredCARET.rf.train)
MAE.rf.train
# Results in: 3.78
# Plot ypred vs yactual of training data
plot(yactual.train, ypredCARET.rf.train,
xlab="Observed BP train set", ylab="Predicted BP train set",
pch=19, xlim=c(0, ceiling(max(yNN)*1.1)), ylim=c(0, ceiling(max(yNN)*1.1)),main="Prediction error training set (RandomForest model)")
abline(0,1, col='red')
text(200,ceiling(max(yNN)*1.1),paste("RMSE: ",RMSE.rf.train))
text(200,ceiling(max(yNN)*1.1)-50,paste("MAE: ",MAE.rf.train))
motifsize = 4
g <- barabasi.game(n=15,directed = T)
#erdos.renyi.game # randomn
plot(g)
motifs(g, motifsize)
which(motifs(g, motifsize)>=1)
# create the motif MINUS ONE BECAUSE THESE FACKERS START AT 0; THIS IS R NOT FUCKING C++
#plot(graph.isocreate(size=3, number=3-1,directed = T))
graph = g
# Find network motifs in the graph "graph":
mygraphmotifs <- graph.motifs(graph, motifsize)
# Find which motifs occur:
for (i in 1:length(mygraphmotifs))
{
if(is.na(mygraphmotifs[i])){next}
motif <- mygraphmotifs[i]
if (motif > 0) # There are some occurrences of this motif
{
print(i)
# Find out what the motif looks like:
motifgraph <- graph.isocreate(size=motifsize, number=i-1, directed=TRUE)
edges <- E(motifgraph)
plot(motifgraph)
text(-1,-1,paste("This motif occurs",motif,"times."))
print(paste("This motif occurs",motif,"times:"))
print(edges)
}
}
library(igraph)
motifsize = 4
g <- barabasi.game(n=15,directed = T)
#erdos.renyi.game # randomn
plot(g)
motifs(g, motifsize)
which(motifs(g, motifsize)>=1)
# create the motif MINUS ONE BECAUSE THESE FACKERS START AT 0; THIS IS R NOT FUCKING C++
#plot(graph.isocreate(size=3, number=3-1,directed = T))
graph = g
# Find network motifs in the graph "graph":
mygraphmotifs <- graph.motifs(graph, motifsize)
# Find which motifs occur:
for (i in 1:length(mygraphmotifs))
{
if(is.na(mygraphmotifs[i])){next}
motif <- mygraphmotifs[i]
if (motif > 0) # There are some occurrences of this motif
{
print(i)
# Find out what the motif looks like:
motifgraph <- graph.isocreate(size=motifsize, number=i-1, directed=TRUE)
edges <- E(motifgraph)
plot(motifgraph)
text(-1,-1,paste("This motif occurs",motif,"times."))
print(paste("This motif occurs",motif,"times:"))
print(edges)
}
}
motifs(g, motifsize)
plot(graph.isocreate(size=3, number=3-1,directed = T))
plot(graph.isocreate(size=3, number=4,directed = T))
plot(graph.isocreate(size=4, number=4,directed = T))
plot(graph.isocreate(size=4, number=3,directed = T))
version()
version
sample_gnp(n = length(V(graph)),p.or.m =length(V(graph)),directed = T,type = "gnm")
#-----------------------------------------------------------------------------------------------------#
#		Block 01		(Install &) Load packages
#-----------------------------------------------------------------------------------------------------#
# Install packages if needed and load, or just load packages
if (!requireNamespace("BiocManager", quietly = TRUE))
install.packages("BiocManager", ask = F)
# Insert all packages in requiredpackages
requiredpackages <-
c("GRENITS",
"igraph",
"corrr",
"GENIE3",
"parmigene",
"RCy3")
for (i in requiredpackages) {
if (!requireNamespace(i, quietly = TRUE))
BiocManager::install(i, ask = F, dependencies = TRUE)
require(as.character(i), character.only = TRUE)
print(i)
}
sample_gnp(n = length(V(graph)),p.or.m =length(V(graph)),directed = T,type = "gnm")
sample_gnp(n = length(V(graph)),p.or.m =length(V(graph)),directed = T)
sample_gnp(n = length(V(graph)),p =length(V(graph)),directed = T)
sample_gnp(n = length(V(graph)),p =length(V(graph)),directed = T)
sample_gnp(n = 20,p =length(V(graph)),directed = T)
sample_gnp(n = 20,p =20,directed = T)
a=1
a
a=1:100
a
b=a^2
plot(a,b)
prcomp(b)
plot(prcomp(b))
x=1:100
plot(sin(x))
x=1:1000
x=x/10
plot(sin(x))
x=1:1000
x=x/100
plot(sin(x))
x
getwd()
a="thisistext"
write(a,file = "tet.txt")
a={#-----------------------------------------------------------------------------------------------------#
# 									(Install &) Load packages
#-----------------------------------------------------------------------------------------------------#
# clear workspace & garbage collect
if(Sys.info()[6]=="RR"){
rm(list=ls())
gc()
#memory.limit(size = 23900) #(100 mb left, just for safety)
}
# Install packages if needed and load, or just load packages
if (!requireNamespace("BiocManager", quietly = TRUE))
install.packages("BiocManager", ask = F)
# Insert all packages in requiredpackages
requiredpackages <- c("gplots","ggplot2","ggfortify")
for( i in requiredpackages){
print(i)
if (!requireNamespace(i, quietly = TRUE))
BiocManager::install(i, ask = F, dependencies=TRUE)
require(as.character(i),character.only = TRUE)
}
}
a
a="#-----------------------------------------------------------------------------------------------------#
# 									(Install &) Load packages
#-----------------------------------------------------------------------------------------------------#
# clear workspace & garbage collect
if(Sys.info()[6]=="RR"){
rm(list=ls())
gc()
#memory.limit(size = 23900) #(100 mb left, just for safety)
}
# Install packages if needed and load, or just load packages
if (!requireNamespace("BiocManager", quietly = TRUE))
install.packages("BiocManager", ask = F)
# Insert all packages in requiredpackages
requiredpackages <- c("gplots","ggplot2","ggfortify")
for( i in requiredpackages){
print(i)
if (!requireNamespace(i, quietly = TRUE))
BiocManager::install(i, ask = F, dependencies=TRUE)
require(as.character(i),character.only = TRUE)
}
"
a
a=
"
#-----------------------------------------------------------------------------------------------------#
# 									(Install &) Load packages
#-----------------------------------------------------------------------------------------------------#
# clear workspace & garbage collect
if(Sys.info()[6]=="RR"){
rm(list=ls())
gc()
#memory.limit(size = 23900) #(100 mb left, just for safety)
}
# Install packages if needed and load, or just load packages
if (!requireNamespace("BiocManager", quietly = TRUE))
install.packages("BiocManager", ask = F)
# Insert all packages in requiredpackages
requiredpackages <- c("gplots","ggplot2","ggfortify")
for( i in requiredpackages){
print(i)
if (!requireNamespace(i, quietly = TRUE))
BiocManager::install(i, ask = F, dependencies=TRUE)
require(as.character(i),character.only = TRUE)
}
"
a=
'
#-----------------------------------------------------------------------------------------------------#
# 									(Install &) Load packages
#-----------------------------------------------------------------------------------------------------#
# clear workspace & garbage collect
if(Sys.info()[6]=="RR"){
rm(list=ls())
gc()
#memory.limit(size = 23900) #(100 mb left, just for safety)
}
# Install packages if needed and load, or just load packages
if (!requireNamespace("BiocManager", quietly = TRUE))
install.packages("BiocManager", ask = F)
# Insert all packages in requiredpackages
requiredpackages <- c("gplots","ggplot2","ggfortify")
for( i in requiredpackages){
print(i)
if (!requireNamespace(i, quietly = TRUE))
BiocManager::install(i, ask = F, dependencies=TRUE)
require(as.character(i),character.only = TRUE)
}
'
a
write(x = a,file = "test.txt")
requiredpackages =
c(
"WikidataQueryServiceR",
"ggplot2",
"backports",
"rJava",
"rcdk",
"pls",
"randomForest",
"gplots",
"curl",
"data.table",
"caret",
"ggfortify",
"tidyverse"
)
requiredpackages
setwd("D:\\Github\\R-project-blank\\R-project)
setwd("D:\\Github\\R-project-blank\\R-project")
ls()
getwd()
source("./R/1-load.R")
ls
ls()
ls(".//")
ls("./")
list.files(path = "./")
list.files(path = "./input/d")
list.files(path = "./input/")
